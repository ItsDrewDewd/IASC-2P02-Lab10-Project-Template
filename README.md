---
title: Final Project
subtitle: > 
    Effectiveness of Natural Language Processing and Machine Learning Algorithms in Text Analysis: A Case Study of Voyant-Tools
author: Name
summary: > 
    This is a case study of Voyant-tools carried out to measure the effectiveness of AI-aided text analysis when used in digital humanities. 
category: Digital humanities
template: final_project
fontfamily: sans-serif
class: IASC 2P02
professor: Name
school: Name
date: current date
---

# Abstract

    The application of Artificial Intelligence (AI) in digital text analysis via Natural Language Processing (NLP) and Machine Learning (ML) algorithms is a current trend in digital humanities research. With the increased application of AI in the analysis of large datasets containing textual data, the need to evaluate the effectiveness of tools based on AI in achieving research goals is unavoidable. Voyant-Tools, a tool for analyzing textual data, utilizes NLP and ML. Therefore, this study evaluates the effectiveness of Voyant-Tools in supporting digital humanities research by analyzing a text from the nineteenth century titled "The Underground Railroad from Slavery to Freedom: A Comprehensive History." Close reading is also applied to the same text, with a reader identifying key themes and topics using keywords from the text. The results prove the effectiveness of Voyant-Tools for text analysis in digital humanities research, though limitations concerning accuracy are noticeable when compared to close reading.

# Introduction

    Research concerning digital humanities keeps growing, thus increasing the use of AI in the field. Therefore, effective text analysis tools have become imperative. One such tool that has gained popularity in the digital humanities research field is Voyant-Tools. A text analysis tool that utilizes AI via ML algorithms and NLP. Voyant-Tools is among the most promising methods for analyzing large volumes of text data. Nevertheless, there is a need to evaluate the effectiveness of AI-based machine learning algorithms and natural language processing in text analysis compared to traditional text analysis methods in digital humanities research, with Voyant-Tools as a case study. 

    Literature on whether AI, ML, and NLP are effective in text analysis exists. For instance, Ganguli et al.'s evaluation proves NLP's efficiency in textual data analysis with a 96 percent accuracy rate (776). When the accuracy is at 96 percent, the results of tasks conducted with supervised ML models meet almost all the researcher's needs. Moreover, Tahayori et al. find the application of ML algorithms and NLP efficient enough to be recommended for use in emergency triage notes to predict patient disposition due to the high level of accuracy (482). The study establishes that natural language processing returns an accuracy of 83 percent when analyzing textual data. Hence, existing literature discussing the effectiveness of AI and NLP classifies these technologies as very efficient. 

    Literature focused on the efficiency and success rate of Voyant-Tools in text analysis concludes that the tool is very efficient when used properly. According to Gregory et al., the results generated by using Voyant-Tools in archival data collection created deeper knowledge of the collection for the information professionals who steward the collection and future researchers of the collection (13). Therefore, Voyant-Tools was effectively applied to aid data collectors in understanding their archival data. The efficiency of Voyant-Tools is dependent on the tool users' ability to define the parameters of their work so that proper filtration that ensures accuracy is applied (Alhudithi). To achieve accurate text analysis in Voyant-Tools, users must clean up their data with functionalities like stop word lists. Conclusively, though Voyant-Tools is efficient in text analysis, the efficiency may depend on a researcher's proficiency.

    This work compares the effectiveness of AI as used in text analysis via NLP and ML algorithms to traditional text analysis methods in the context of digital humanities research. The comparison is achieved by experimenting using a nineteenth-century archival text titled "The Underground Railroad from Slavery to Freedom: A Comprehensive History," which is analyzed using both Voyant-Tools and close reading. Post the experiment, an examination of the patterns, themes, and topics identified by each method is conducted, followed by a comparison of the effectiveness of both methods in identifying the patterns, themes, and topics. Hypothetically, applying AI through NLP and ML in text analysis, specifically when using Voyant-Tools, is more effective in identifying patterns, themes, and topics in texts for digital humanities research than traditional text analysis methods. The findings of this study will provide insights into the effectiveness of AI when used to analyze text in digital humanities research, thus helping researchers, scholars, and students in the field to make informed decisions about which methods to use for their research. 
 



# Methodology

    This study utilizes an experimental research design wherein a randomized controlled trial is employed to examine the comparative effectiveness of AI-based text analysis with close reading in digital humanities research.

## Apparatus and Materials

    The materials used for this work include the Voyant Tools, RawGraphs, and a corpus of text from the Project Gutenberg website titled 'The Underground Railroad from Slavery to Freedom: A comprehensive history' by Reginald Arthur Bray. The link to the text material used is as follows; https://www.gutenberg.org/ebooks/49038. Voyant-Tools uses ML Algorithms and NLP and was thus used in the corpus's AI-aided text analysis. RawGraphs was used to visualize data obtained from the traditional text analysis method of close reading. 

## Procedures

    The first section of the experiment involved analyzing the prementioned corpus in Voyant-Tools. The corpus was first downloaded from Project Gutenberg and saved locally in plain text format. It was then uploaded to Voyant-Tools. According to Sinclair and Rockwell, Voyant-Tools preprocesses corpuses immediately after upload based on default preferences. Therefore, upon uploading the corpus used in this study to Voyant-Tools, it was automatically preprocessed; thus, a dashboard was generated, illustrated in fig. 1. The preprocessing involves removing numerical data and punctuation marks, and stop words from the corpus. Additionally, all the terms used in the corpus are transformed back to their most natural forms. The findings obtained in this procedure are discussed in detail in the results section.

    The other section of the experiment involved analyzing the text from Project Gutenberg through close reading. To establish the themes, topics, and sentiments in the textual data, all the words that the readers established as key were recorded in tabular form, as illustrated in fig. 2. The data generated after close reading the whole document was then uploaded to RawGraphs for data visualization. The findings of the visuals generated in RawGraphs were for comparison with the conclusive data obtained from visualizations in Voyant-Tools. 
 
![Fig. 1. A dashboard generated after the corpus was preprocessed in Voyant-Tools.](Voyant-Tools%20dashboard.png)

![Fig. 2. A tabular representation of close reading data.](Close%20reading%20data%20collection.png)




# Results 

    Voyant-Tools provided a dashboard upon processing the corpus, summarizing the AI-aided text analysis. Sinclair and Rockwell state that the mini tools in Voyant-Tools, which include cirrus, links, trends, summary, and terms, are crucial to providing more insights regarding the text analysis. The summary tool provided the insights in fig. 3; the corpus has 190,358 words and 13,463 unique word forms. The summary functionality also listed the most frequent words in the corpus, including slave (887), fugitive (835), states (647), and law (629).
 
![Fig. 3. An illustration of the summary of the corpus as generated in Voyant-Tools.](Fig.%203.%20summary.png)


    The Cirrus tool in Voyant-Tools was crucial in generating word clouds critical in text analysis visualization. Sinclair & Rockwell state that the size of words in the word cloud represents the frequency with which a word appears in the corpus. Therefore, the largest words illustrated in fig. 4 were the most frequent in the corpus. These words included slave, fugitives, slavery, law, states, and work. 

    The terms tool listed all the unique word forms in the corpus alongside the frequency at which they appeared in the corpus. The unique words in the terms section are selectable to manipulate other tools concerning selected unique words. The links tool in Voyant-Tool helps establish the connection between words generated in the textual data analysis (Sinclair and Rockwell). Fig. 5 illustrates the co-occurrence of words including fugitive, slave, slavery, law, power, railroad, and court, helping identify the relationship between the words. The co-occurrence suggests the main theme and topic of the corpus. 

![Fig. 4. The word cloud generated in the cirrus tool.](Cirrus.png)
 
![Fig. 5. A links illustration that identifies co-occurrence of terms in the corpus.](links.png)

    Trends tool in Voyant-Tools generates trend lines in a graph of document segments against terms frequencies. Upon selecting terms in the terms list, a trends graph illustrated in fig. 6 was generated, showing how the selected most occurring words in the corpus are distributed across the textual data. 
 
![Fig. 6. The trends graph showing how words are distributes across the corpus.](trends.png)

    The Voyant-Tools, with its mini tools, including Cirrus, Terms, Links, and Trends, facilitate gaining further insights about the corpus. The Cirrus tool was crucial in generating a word cloud in this study. The Links tool helped identify words that frequently appear alongside the selected terms. The Trends tool aided in establishing patterns of occurrence for specific terms in different segments of the corpus, thus helping identify themes and topics in the corpus.

    Data collected during close reading was tabulated, as illustrated earlier in this work. The data in a CSV file was uploaded to RawGraphs for visualization purposes. When choosing a chart to illustrate the data, a pie chart was the most suitable, and the terms and count values from the uploaded data were used for the chart variables. The most occurrent terms in the corpus are represented by the biggest pies in the generated graph, as shown in fig. 7.
 
![Fig. 7. A graphical representation of data obtained from close reading.](close%20read%20data.png)




# Discussion 

    The study's findings offer significant insights into the effectiveness of AI's application via ML and NLP in text analysis compared to the traditional text analysis method of close reading. According to McGann and Samuels, traditional text analysis is often slow and leads to misunderstandings. Notably, the variance in frequently used terms in the text is insignificant, thus speed and accuracy are the only things that stood out in the AI-aided analysis. The case study of Voyant-Tools shows that NLP and ML algorithms, AI technologies used in the tool, provide substantial benefits in speed and accuracy during text analysis, particularly when handling large volumes of text data. Nevertheless, Kirschenbaum notes that applying ML algorithms and NLP has several limitations associated with analyzing large complex data. The results indicate limitations to using AI in textual data analysis, particularly regarding the more nuanced and complex analysis. While the machine learning algorithms used in these tools efficiently identify patterns and trends in the data, they often need more interpretative skills and a contextual understanding of human researchers. Consequently, AI may need to pay more attention to essential subtleties or cultural nuances critical to analyzing certain texts to achieve full efficiency.

    It is imperative for digital humanities researchers to carefully consider the strengths and limitations of analyzing text using AI technologies when selecting the appropriate method for their research. Close reading may be more suitable than AI-based analysis based on the type of textual data being analyzed; alternatively, combining technology and traditional analyzing methods may be more suitable in some cases (Cox). Although tools based on NLP and ML algorithms can provide valuable insights and save time and effort, they should be perceived as something other than a replacement for human expertise and critical analysis. This study underscores the need for continued research and progressive development of AI-based text analysis tools in digital humanities to ensure maximum efficiency. The focus should be on enhancing their ability to capture the complexity and nuance of human language and cultural expression. Conclusively, the current AI technologies should be combined with traditional text analysis methods to optimize the benefits of both approaches.


# Conclusion

   The study sheds light on the effectiveness of AI-based text analysis tools that utilize natural language processing and machine learning algorithms in digital humanities research. The findings suggest that these tools offer significant advantages in terms of speed and accuracy, particularly when managing large volumes of textual data. However, they also have certain disadvantages that limit their effectiveness; the case study of Voyant-Tools provides valuable insights into the strengths and limitations of text analysis that AI aids. Notably, further research is necessary to enhance these tools' ability to capture the human language and culture complexities. Overall, the study's findings demonstrate that the application of AI in text analysis through ML and NLP can revolutionize how researchers approach text analysis in digital humanities research., given the associated effectiveness levels. By utilizing tools based on AI technologies, such as NLP and ML, in conjunction with traditional methods, researchers can gain valuable insights into written texts' cultural and historical significance and contribute to a deeper understanding of the complexities of human language and expression.



# Works Cited

    Alhudithi, Ella. Review of Voyant Tools: See through Your Text, 2021.

    Cox, A. M. “Exploring the Impact of Artificial Intelligence and Robots on Higher Education through Literature-Based Design Fictions.” International Journal of Educational Technology in Higher Education, vol. 18, no. 1, 2021, https://doi.org/10.1186/s41239-020-00237-8. 

    Ganguli, Rajive , et al. “Effectiveness of Natural Language Processing Based Machine Learning in Analyzing Incident Narratives at a Mine.” Minerals, vol. 11, no. 7, 2021, p. 776., https://doi.org/10.3390/min11070776. 

    Gregory, Kate, et al. “Voyant Tools and Descriptive Metadata: A Case Study in How Automation Can Compliment Expertise Knowledge.” Journal of Library Metadata, vol. 22, no. 1-2, 2022, pp. 1–16., https://doi.org/10.1080/19386389.2022.2030635. 

    Kirschenbaum, MATTHEW. “What is Digital Humanities and What’s It Doing in English Departments?” Debates in the Digital Humanities, 2012, https://dhdebates.gc.cuny.edu/read/untitled-88c11800-9446-469b-a3be-3fdb36bfbd1e/section/f5640d43-b8eb-4d49-bc4b-eb31a16f3d06. 

    McGann, Jerome, and Lisa Samuels. “Deformance and Interpretation.” Deformance and Interpretation, 2020, http://www2.iath.virginia.edu/jjm2f/old/deform.html. 

    Sinclair, Stéfan, and Geoffrey Rockwell. “Researchers, Teachers, and Learners Seeing New Possibilities with Voyant Tools.” Tesl-Ej, Aug. 2020, http://tesl-ej.org/wordpress/issues/volume24/ej94/ej94m1/. 

    Tahayori, Bahman, et al. “Advanced Natural Language Processing Technique to Predict Patient Disposition Based on Emergency Triage Notes.” Emergency Medicine Australasia, 2020, pp. 480–484., https://doi.org/10.1111/1742-6723.13656. 

